---
layout: post
comments: true
title:  "Questions For 1dv022"
date:   2016-11-9 10:05:44
categories: studies
---

## What do you think of pre-compiling your CSS?

I think that pre-compiling the css is a great time investment in the long run. If you're creating what would be considered a simple and small site, you
might not really feel a difference. In that case, a pre-processor might even be redundant since you'll be done in an hour anyway. However, when creating
a bigger project you it's extremely tiring and annoying having to break the DRY-rule (Don't repeat yourself) over and over again.

Let's look at this example:

{% highlight css %}
body {
    background: green;
}
.button {
    background: green;
}
#wrapper {
    background: red;
}
ul li {
    font-size: 15px;
}
{% endhighlight %}

Here we can see some very basic CSS containing some classes and ID's. Green is acting as the websites primary color. Nothing tedious or annoying about this.. yet.

Let's compare it with how it would look if written in SASS:

{% highlight scss %}
$primaryColor: green;
body {
    background: $primaryColor;
}
.button {
    background: $primaryColor;
}
#wrapper {
    background: red;
}
ul {
    li {
        font-size: 15px;
    }
}
{% endhighlight %}

When looking and comparing these two, the first impression I would get if I knew nothing about pre-processors would be that it just complicates things. I mean, by
looking at the the snippets, the regular CSS looks shorter and more straight forward. However, what happens if your client (or you) wants to change the color scheme
in the middle of a really big project? You'll end up having to copy paste the same css row over and over again. Furthermore, what happens if your client (or you) decides
that the new color scheme isn't quite right either which means you have to change it again? You get the point. This is just one of many pros of using an CSS pre-processor.

This site's CSS was generated by the pre-processor named [Sass](http://sass-lang.com/). Some techniques used:

+ Variable declarations.
+ Nesting.
+ Partials.
+ Imports.

Below is a list of pros and cons of pre-processing CSS:

Pros:

+ Modularizations.

+ Easier to follow the 'DRY' principle.

+ Nesting.

+ Variables.

+ Mixins.

+ Mathematical functions.

+ Joining of files.

Cons:

+ If you want to use a pre-processor in a team project, everybody needs to know how to use one. If someone isn't familiar with it and starts editing the output css, it can cause
problems.

+ Can be a bit hard to learn, especially compared with regular CSS where the way of coding is a bit more straight forward. However, if you have previous experience with programming
you might find pre-processors way of creating CSS more logical, since you can use some syntax you're familiar with.

+ Since CSS is abstracted, this adds another step to updates and changes. This might also affect debugging.

## What do you think of static site generators?

I think static site generators are great, but it's not for every site. One common characteristic of a static site is how fast it loads. In a static site, there's no database queries to
run and furthermore no processing on every request. The entire site consists of static HTML files that's just sitting on a server, waiting to distribute. Web servers are really good at
delivering these static sites which means that when a request is filed, it's served almost instantly.

Another great thing thing with static site generators is the fact that you don't have you deal with installing and maintaining the infrastructure required to run a dynamic site. This can
be hard to work with sometimes, especially when you have more than one server involved. Even if static site generators is a software package with its dependencies too, that's only relevant
at the time you generate your site. Once it's generated, it's there for good. HTML files will remain HTML files and can be migrated and scaled regardless of which server technologies being
used.

However, there's also some disadvantages to using this technology. When you're creating a static site you lose the ability to have real-time data on your site. Static will remain static
for everyone which means that you can't 'tailor' the site to better fit an individual. Furthermore, there's no kind of admin user-interface on a static site. For example, when you are
publishing a post you need to regenerate the site. Many engines have a so-called 'watch' function (I'm currently using 'npm run watch' as I type this) to detect file changes and automatically
regenerate the site. This might however get tricky if your using other devices than a computer.

To summarize, switching to a static site can save both time and money since it doesn't require as much maintenance. These sites are straight forward and reliable, what you see is what you get.
However, it's not meant for every site. Therefore it's important to understand how they work and when they should be used.

## What is robots.txt and how have you configure it for you site?

The robots.txt file is one of the main ways of restricting a search engine of where it can, and can't, go on your website. All of the major search engines supports the basic functionality of
of this file. The file should be put in the root of your domain. For example, if you website is [http://www.mysite.com](#) your robots.txt should be found in [http://www.mysite.com/robots.txt](#).

So how does it work? Well, the first thing a so-called search engine 'spider' (or robot) does when it encounters a new domain is search for the robot.txt document. This tell the
spider where it can, and can't go. As a result, it's very important that you name this file correctly (it's case sensitive). Worth mentioning is that even though you can control where the
robot can go, you can't remove your site from search engine using the robots.txt. If the search engine find enough links to your domain, it will be included. However, the search engine will
not know what to do with that domain.

This is what my robots.txt looks like:

{% highlight text %}

User-agent: *
Disallow:

{% endhighlight %}

The user-agent identifies a specific spider. For instance, if I want to target google's spider 'Googlebot', I simply adjust the row to say 'User-agent: Google'. In my case, I use the character
* which allows all search engines to crawl my entire site. The Disallow line of the file is the part where you tell the robots what they can and can't access. I chose to not restrict them in any
way. However, if you wanted to disallow all search engines (that's compatible with robots.txt) you'd use the character /. This emphasizes how important it is to be careful when setting this up,
one single character is the difference between full access, and no access at all.

You can also set up individual rules for different bots, like this:

{% highlight java %}
//Grant Googlebot access of everything
User-agent: Google
Disallow:
//Don't let the Evil Bot in!
User-agent: EvilBot
Disallow: /

{% endhighlight %}

##Alright, enough about robots. What's humans.txt?

The humans.txt is a simple way of 'humanizing' (if you will) a website. It's a simple way to see the team behind a website and which tools and language that was being used. My human.txt file
looks like this:

{% highlight text %}

/* TEAM */

Your title: Carl Viberg
Site: Carlv@hotmail.se

/* THANKS */
Name: All LNU lecturers.

/* SITE */
Last update: 2016/11/17
Standards: HTML5, CSS
Components: Webstorm, Jekyll, Sass


{% endhighlight %}

As you can see, this file is very straight forward.

## How did you implement comments to blog posts?

In order to create a comment system for my website I used something called Disqus, which is a networked community platform. It automatically adapts to your sites design and color scheme, but
you can also change it manually if you'd like. You can also use Disqus to earn money, since you can include native ads to generate revenue. That's not important whatsoever in my case though,
since my site isn't going to have a large audience. This comment system is easy to install, I just went to their [website](https://disqus.com/), filled out some information to make the
platform adapt to my site (URLs etc) and then added the code snippet to my posts. Straight forward and simple!

## What is Open Graph and how do you make use of it?

The Open Graph technology was first introduced by Facebook in 2010. It allows integration between Facebook, with its user data and a website. Using Open Graph is simple, you use their tags
'og' to specify what elements of your page you'd like to show when someone share's your website. By specifying this, Facebook doesn't have to guess which elements to use.


{% if page.comments %}
<div id='disqus_thread'></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = page.url; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = page.identifier; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//carlv.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href='https://disqus.com/?ref_noscript'>comments powered by Disqus.</a></noscript>
{% endif %}

[jekyll]:      http://jekyllrb.com
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-help]: https://github.com/jekyll/jekyll-help
